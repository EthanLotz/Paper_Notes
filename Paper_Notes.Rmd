---
title: "Paper_Notes"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
body {
text-align: justify}
</style>

### Watson & Crick, 1953

- Watson, J. D., and F. H. Crick. "Molecular Structure of Nucleic Acids;a Structure for Deoxyribose Nucleic Acid." Nature 171, no. 4356 (April 25, 1953): 737-38

**Summary:**

In this landmark paper, Watson and Crick propose a novel structure for deoxyribonucleic acid (DNA). Prior to their work, others structures were proposed including a model consisting of three intertwined chains, with phosphate groups located internally near the fiber axis and bases on the outside (Pauling and Corey, 1953). It is argued that this structure cannot work because negatively charged phosphate groups in close proximity would repel each other. Furthermore, the van der Waals forces generated by this model would be too small to hold it together. Instead, Watson and Crick propose a model consisting of two helical chains, coiled around the same axis with bases on the inside and phosphates on the outside. Although this structure is based on a model proposed by Furberg (1952), Watson and Crick's novelty stems from the manner in which the two helices are held together. A single base from one chain is bound to a single base from the opposite strand via hydrogen bonds. In order for this to work, one of the pair must have an underlying purine structure while the other must have an underlying pyrimidine structure. Assuming the bases only occur in the structure in the most plausible tautomeric form, only specific pairs of bases can bond together (i.e. adenine (purine) with thymine (pyrimidine), and guanine (purine) with cytosine (pyrimidine). I am both surprised and amazed by this paper. It is incredible that based only on certain assumptions, Watson and Crick were able to deduce the structure of DNA with exact dimensions. However, I am surprised that it is so short. Although they do say full details will be published elsewhere, it is hard to imagine editorials proposing radical shifts in certain scientific paradigms could be published without supporting evidence, data, or detailed calculations.

### Tumor Analysis Best Practices Working Group, 2004

- Tumor Analysis Best Practices Working Group. "Expression Profiling-Best Practices for Data Generation and Interpretation in Clinical Trials." Nature Reviews. Genetics 5, no. 3 (March 2004): 229-37.

**Summary:**

The review written by the Tumor Analysis Best Practices Working Group attempts to provide recommendations for standardizing experimental design, probe-set algorithms, signal/noise assessments, and biostatistical methods for microarray technologies. It is important to develop a set of best practices since microarray technologies are widely used in clinical trials, both for differential diagnoses and monitoring of pharmacological efficacy. Their first discussion point regards the importance of appropriate experimental design. Replication in cross-sectional studies is important since the source of biological variability can confound many results. Preclinical animal testing may only need a few replicates whereas human samples require considerably more individuals per group due to their genetic heterogeneity. Tissue/cell heterogeneity is a major confounding variable in many micro-array experiments. The Working Group suggests normalization techniques based on bioinformatics methods, or isolating pure cell populations for expression profiling. Furthermore, a longitudinal experimental design provides greater power at lower numbers of replicates since each subject serves as their own control. Technical variability is the next discussion point the Working Group reviews. Methods for checking the purity of isolated RNA have been used for over three decade. The Working Group recommends that samples that do not meet the established criteria should be discarded. In addition, the development of standards would require all laboratories to use the same control RNA solution before data could be easily compared. The last point discusses the analysis and interpretation of collected data. Much of the debate in the field of bioinformatics about microarray interpretation revolves around signal/noise ratios. The Working Group states that each project will have its own signal/noise optimum, and, ideally, a signal/noise ratio should be optimized for each project. The recommendations the working group suggests should be followed and it would be interesting to read a follow-up to this review as it was published in 2004.

### Lipshutz et al. 1999
- Lipshutz, R. J., S. P. Fodor, T. R. Gingeras, and D. J. Lockhart. "High Density Synthetic Oligonucleotide Arrays." Nature Genetics 21, no. 1 Suppl (January 1999): 20-24.

**Summary:**

This review article describes the fabrication, design, and specific applications of high-density synthetic oligonucleotide arrays created by the company Affymetrix, Inc. It should be noted that the review was written by employees of Affymetrix, Inc. The ability for these arrays to come to fruition was due to two key technologies: 1) the fabrication of hundreds of thousands of polynucleotides at high spatial resolution in precise locations on a surface and 2) the optimization of laser confocal fluorescence scanning. Affymetrix uses photolithography, the same method used to fabricate integrated circuits for electronics, to deposit oligonucleotide probes that match parts of the sequence of interest of known or predicted open readings frames of genetic material. To fabricate the array, light and light-sensitive masking agents are used to generate a sequence one nucleotide at a time across the entire array. Each applicable probe is selectively unmasked prior to bathing the array in a solution of a single nucleotide, then a masking reaction takes place and the next set of probes are unmasked in preparation for a different nucleotide exposure. After many repetitions, the sequences of every probe become fully constructed. One of the keys to this process is through probe redundancy where each array is constructed from a series of mismatch (MM) control probes that are identical to their perfect match (PM) probes, allowing some assessment of non-specific binding and performance of the probes. Overall, the Affymetrix oligonucleotide arrays allow robust measures of gene expression. The resolution allows one hundred million non-overlapping 30-mer probes spanning the entire human genome to fit on a 2x2 cm array. Furthermore, these arrays are designed based on sequence information, which eliminates the need for physical intermediates such as clones, PCR products, or cDNAs, which drastically reduces many sources of potential error.

### Atman et al. 2014

- Altman, Naomi, and Martin Krzywinski. "Points of Significance: Sources of Variation." Nature Methods 12, no. 1 (December 30, 2014): 5-6.

**Summary:**

The article discusses the double-edged sword that is variation in biological based experiments. Although general consensus is to minimize experimental variability, some variability needs to be maintained to allow generalization of the results. The authors argue that experimental control, randomization, blocking, and replication are the tools that allow replicable and meaningful results. Variability affects replication of experimental results (internal validity) which, in turn, affects the generalizability to a larger population (external validity). A well-designed experiment is a compromise between internal and external validity, and, with careful experimental design, the impact of biological variability can be reduced. Genotype and gender are examples of sources of variability that are under complete experimental control, while other sources of variability, such as diet, temperature, and housing, are under partial experimental control. So-called "noise" factors that cannot be controlled, or are unknown, can be handled by randomization, replication, and blocking. Randomization is employed to avoid biasing experimental results. Replication is needed to increase the precision with which a sample is characterized (e.g. standard error mean). However, the biological variability can be underestimated when measurements are an average of a large number of contributing factors. Blocking on a noise variable removes its effect by taking a difference of two measurements that share the same value of the noise, effectively isolating this variability source. Technical variability, such as reagents, measurement platforms, and personnel, also contribute to the overall variation. Steps should be taken to reduce sources of variation that are nuisance factors while maintaining the sources of variation required to assess how much effects vary in the generalizable population. Whereas the former should be minimized to optimize the power of the experiment, the latter need to be sampled and quantified so that results can be generalized while maintaining the integrity of the estimate uncertainty.