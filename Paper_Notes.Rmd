---
title: "Paper_Notes"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
body {
text-align: justify}
</style>

### Watson & Crick, 1953

- Watson, J. D., and F. H. Crick. "Molecular Structure of Nucleic Acids;a Structure for Deoxyribose Nucleic Acid." Nature 171, no. 4356 (April 25, 1953): 737-38

**Summary:**

In this landmark paper, Watson and Crick propose a novel structure for deoxyribonucleic acid (DNA). Prior to their work, others structures were proposed including a model consisting of three intertwined chains, with phosphate groups located internally near the fiber axis and bases on the outside (Pauling and Corey, 1953). It is argued that this structure cannot work because negatively charged phosphate groups in close proximity would repel each other. Furthermore, the van der Waals forces generated by this model would be too small to hold it together. Instead, Watson and Crick propose a model consisting of two helical chains, coiled around the same axis with bases on the inside and phosphates on the outside. Although this structure is based on a model proposed by Furberg (1952), Watson and Crick's novelty stems from the manner in which the two helices are held together. A single base from one chain is bound to a single base from the opposite strand via hydrogen bonds. In order for this to work, one of the pair must have an underlying purine structure while the other must have an underlying pyrimidine structure. Assuming the bases only occur in the structure in the most plausible tautomeric form, only specific pairs of bases can bond together (i.e. adenine (purine) with thymine (pyrimidine), and guanine (purine) with cytosine (pyrimidine). I am both surprised and amazed by this paper. It is incredible that based only on certain assumptions, Watson and Crick were able to deduce the structure of DNA with exact dimensions. However, I am surprised that it is so short. Although they do say full details will be published elsewhere, it is hard to imagine editorials proposing radical shifts in certain scientific paradigms could be published without supporting evidence, data, or detailed calculations.

### Tumor Analysis Best Practices Working Group, 2004

- Tumor Analysis Best Practices Working Group. "Expression Profiling-Best Practices for Data Generation and Interpretation in Clinical Trials." Nature Reviews. Genetics 5, no. 3 (March 2004): 229-37.

**Summary:**

The review written by the Tumor Analysis Best Practices Working Group attempts to provide recommendations for standardizing experimental design, probe-set algorithms, signal/noise assessments, and biostatistical methods for microarray technologies. It is important to develop a set of best practices since microarray technologies are widely used in clinical trials, both for differential diagnoses and monitoring of pharmacological efficacy. Their first discussion point regards the importance of appropriate experimental design. Replication in cross-sectional studies is important since the source of biological variability can confound many results. Preclinical animal testing may only need a few replicates whereas human samples require considerably more individuals per group due to their genetic heterogeneity. Tissue/cell heterogeneity is a major confounding variable in many micro-array experiments. The Working Group suggests normalization techniques based on bioinformatics methods, or isolating pure cell populations for expression profiling. Furthermore, a longitudinal experimental design provides greater power at lower numbers of replicates since each subject serves as their own control. Technical variability is the next discussion point the Working Group reviews. Methods for checking the purity of isolated RNA have been used for over three decade. The Working Group recommends that samples that do not meet the established criteria should be discarded. In addition, the development of standards would require all laboratories to use the same control RNA solution before data could be easily compared. The last point discusses the analysis and interpretation of collected data. Much of the debate in the field of bioinformatics about microarray interpretation revolves around signal/noise ratios. The Working Group states that each project will have its own signal/noise optimum, and, ideally, a signal/noise ratio should be optimized for each project. The recommendations the working group suggests should be followed and it would be interesting to read a follow-up to this review as it was published in 2004.

### Lipshutz et al. 1999
- Lipshutz, R. J., S. P. Fodor, T. R. Gingeras, and D. J. Lockhart. "High Density Synthetic Oligonucleotide Arrays." Nature Genetics 21, no. 1 Suppl (January 1999): 20-24.

**Summary:**

This review article describes the fabrication, design, and specific applications of high-density synthetic oligonucleotide arrays created by the company Affymetrix, Inc. It should be noted that the review was written by employees of Affymetrix, Inc. The ability for these arrays to come to fruition was due to two key technologies: 1) the fabrication of hundreds of thousands of polynucleotides at high spatial resolution in precise locations on a surface and 2) the optimization of laser confocal fluorescence scanning. Affymetrix uses photolithography, the same method used to fabricate integrated circuits for electronics, to deposit oligonucleotide probes that match parts of the sequence of interest of known or predicted open readings frames of genetic material. To fabricate the array, light and light-sensitive masking agents are used to generate a sequence one nucleotide at a time across the entire array. Each applicable probe is selectively unmasked prior to bathing the array in a solution of a single nucleotide, then a masking reaction takes place and the next set of probes are unmasked in preparation for a different nucleotide exposure. After many repetitions, the sequences of every probe become fully constructed. One of the keys to this process is through probe redundancy where each array is constructed from a series of mismatch (MM) control probes that are identical to their perfect match (PM) probes, allowing some assessment of non-specific binding and performance of the probes. Overall, the Affymetrix oligonucleotide arrays allow robust measures of gene expression. The resolution allows one hundred million non-overlapping 30-mer probes spanning the entire human genome to fit on a 2x2 cm array. Furthermore, these arrays are designed based on sequence information, which eliminates the need for physical intermediates such as clones, PCR products, or cDNAs, which drastically reduces many sources of potential error.

### Altman et al. 2014

- Altman, Naomi, and Martin Krzywinski. "Points of Significance: Sources of Variation." Nature Methods 12, no. 1 (December 30, 2014): 5-6.

**Summary:**

The article discusses the double-edged sword that is variation in biological based experiments. Although general consensus is to minimize experimental variability, some variability needs to be maintained to allow generalization of the results. The authors argue that experimental control, randomization, blocking, and replication are the tools that allow replicable and meaningful results. Variability affects replication of experimental results (internal validity) which, in turn, affects the generalizability to a larger population (external validity). A well-designed experiment is a compromise between internal and external validity, and, with careful experimental design, the impact of biological variability can be reduced. Genotype and gender are examples of sources of variability that are under complete experimental control, while other sources of variability, such as diet, temperature, and housing, are under partial experimental control. So-called "noise" factors that cannot be controlled, or are unknown, can be handled by randomization, replication, and blocking. Randomization is employed to avoid biasing experimental results. Replication is needed to increase the precision with which a sample is characterized (e.g. standard error mean). However, the biological variability can be underestimated when measurements are an average of a large number of contributing factors. Blocking on a noise variable removes its effect by taking a difference of two measurements that share the same value of the noise, effectively isolating this variability source. Technical variability, such as reagents, measurement platforms, and personnel, also contribute to the overall variation. Steps should be taken to reduce sources of variation that are nuisance factors while maintaining the sources of variation required to assess how much effects vary in the generalizable population. Whereas the former should be minimized to optimize the power of the experiment, the latter need to be sampled and quantified so that results can be generalized while maintaining the integrity of the estimate uncertainty.

### Bolstad et al. 2003

- Bolstad, Irizarray, Astrand, Speed. "A Comparison of Normalization Methods for High Density Oligonucleotide Array Data Based on Variance and Bias." Bioinformatics 19, no.2 (January 22, 2003): 185-93.

**Summary:**

The article compares the performance of three methods of normalization for use when analyzing high density oligonucleotide arrays. It is important to remove sources of variation between arrays of non-biological origin when running experiments that involve multiple high density oligonucleotide arrays. Normalization is a process for reducing unwanted variability. For these arrays, oligonucleotides of 25 base pairs in length are used to probe genes. There are two types of probes: 1) perfect match (PM) probes that match a target sequence exactly and 2) mismatch (MM) probes, which differ from the reference probes only by a single base in the center of the sequence. The normalization methods are performed at the probe intensity level, and are referred to as complete data methods because they make use of data from all arrays in an experiment to form the normalizing relation. These methods are then compared to two methods that make use of a baseline array. The first method is Cyclic loess. It is an approach based upon the idea of the M versus A plot, where M is the difference in log expression values and A is the average of the log expression values. The second method is contrast. The contrast based method is another extension of the M versus A method. Quantile normalization is the last method where the goal is to make the distribution of probe intensities for each array in a set of arrays the same. The results of the study suggest that all three of the complete data methods reduced the variation of the probeset measure across a set of arrays to a greater degree than their reference methods. However, the quantile method performed favorably across all experiments, both in terms of speed and when using their proposed variance and bias criteria.

### Cleveland et al. 1988

- Cleveland, William S, and Susan J Devlin. "Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting." Journal of the American Statistical Association 83, no. 403 (1988): 596-610. - Loess regression. Concept, statistics.

**Summary:**

The article discusses locally weighted regression (loess). It is a way of estimating a regression surface through a multivariate smoothing procedure, fitting a function of the independent variables locally and in a moving fashion analogous to how a moving average is computed for a time series. Interestingly, the term loess is derived from the word meaning a deposit of fine clay or silt along river valleys; in a vertical cross-section of Earth, a loess would appear as a narrow, curve-like stratum running through the section. A much wider class of smooth functions can be estimated using local fitting. The applications in the present article illustrate three major uses of the local-fitting methodology. The first is simply to provide an exploratory graphical tool; graphing smooth surfaces that are fitted to the data can give insight into the behavior of the data and help choose parametric models. The second is to provide additional regression diagnostics to check the adequacy of parametric models fitted to the data. The third is to use the loess estimate as the estimated regression surface, without resorting to a parametric class of functions. Other methodologies are discussed, which include multivariate smoothing, the statistical properties of loess that are analogous to those used in the least-squares fitting of parametric functions, several graphical methods that are useful tools for understanding loess estimates and checking assumptions on which the estimation procedure is based, and the M plot.
