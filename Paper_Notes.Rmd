---
title: "Paper_Notes"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
body {
text-align: justify}
</style>

### Watson & Crick, 1953

- Watson, J. D., and F. H. Crick. "Molecular Structure of Nucleic Acids;a Structure for Deoxyribose Nucleic Acid." Nature 171, no. 4356 (April 25, 1953): 737-38

**Summary:**

In this landmark paper, Watson and Crick propose a novel structure for deoxyribonucleic acid (DNA). Prior to their work, others structures were proposed including a model consisting of three intertwined chains, with phosphate groups located internally near the fiber axis and bases on the outside (Pauling and Corey, 1953). It is argued that this structure cannot work because negatively charged phosphate groups in close proximity would repel each other. Furthermore, the van der Waals forces generated by this model would be too small to hold it together. Instead, Watson and Crick propose a model consisting of two helical chains, coiled around the same axis with bases on the inside and phosphates on the outside. Although this structure is based on a model proposed by Furberg (1952), Watson and Crick's novelty stems from the manner in which the two helices are held together. A single base from one chain is bound to a single base from the opposite strand via hydrogen bonds. In order for this to work, one of the pair must have an underlying purine structure while the other must have an underlying pyrimidine structure. Assuming the bases only occur in the structure in the most plausible tautomeric form, only specific pairs of bases can bond together (i.e. adenine (purine) with thymine (pyrimidine), and guanine (purine) with cytosine (pyrimidine). I am both surprised and amazed by this paper. It is incredible that based only on certain assumptions, Watson and Crick were able to deduce the structure of DNA with exact dimensions. However, I am surprised that it is so short. Although they do say full details will be published elsewhere, it is hard to imagine editorials proposing radical shifts in certain scientific paradigms could be published without supporting evidence, data, or detailed calculations.

### Tumor Analysis Best Practices Working Group, 2004

- Tumor Analysis Best Practices Working Group. "Expression Profiling-Best Practices for Data Generation and Interpretation in Clinical Trials." Nature Reviews. Genetics 5, no. 3 (March 2004): 229-37.

**Summary:**

The review written by the Tumor Analysis Best Practices Working Group attempts to provide recommendations for standardizing experimental design, probe-set algorithms, signal/noise assessments, and biostatistical methods for microarray technologies. It is important to develop a set of best practices since microarray technologies are widely used in clinical trials, both for differential diagnoses and monitoring of pharmacological efficacy. Their first discussion point regards the importance of appropriate experimental design. Replication in cross-sectional studies is important since the source of biological variability can confound many results. Preclinical animal testing may only need a few replicates whereas human samples require considerably more individuals per group due to their genetic heterogeneity. Tissue/cell heterogeneity is a major confounding variable in many micro-array experiments. The Working Group suggests normalization techniques based on bioinformatics methods, or isolating pure cell populations for expression profiling. Furthermore, a longitudinal experimental design provides greater power at lower numbers of replicates since each subject serves as their own control. Technical variability is the next discussion point the Working Group reviews. Methods for checking the purity of isolated RNA have been used for over three decade. The Working Group recommends that samples that do not meet the established criteria should be discarded. In addition, the development of standards would require all laboratories to use the same control RNA solution before data could be easily compared. The last point discusses the analysis and interpretation of collected data. Much of the debate in the field of bioinformatics about microarray interpretation revolves around signal/noise ratios. The Working Group states that each project will have its own signal/noise optimum, and, ideally, a signal/noise ratio should be optimized for each project. The recommendations the working group suggests should be followed and it would be interesting to read a follow-up to this review as it was published in 2004.

### Lipshutz et al. 1999
- Lipshutz, R. J., S. P. Fodor, T. R. Gingeras, and D. J. Lockhart. "High Density Synthetic Oligonucleotide Arrays." Nature Genetics 21, no. 1 Suppl (January 1999): 20-24.

**Summary:**

This review article describes the fabrication, design, and specific applications of high-density synthetic oligonucleotide arrays created by the company Affymetrix, Inc. It should be noted that the review was written by employees of Affymetrix, Inc. The ability for these arrays to come to fruition was due to two key technologies: 1) the fabrication of hundreds of thousands of polynucleotides at high spatial resolution in precise locations on a surface and 2) the optimization of laser confocal fluorescence scanning. Affymetrix uses photolithography, the same method used to fabricate integrated circuits for electronics, to deposit oligonucleotide probes that match parts of the sequence of interest of known or predicted open readings frames of genetic material. To fabricate the array, light and light-sensitive masking agents are used to generate a sequence one nucleotide at a time across the entire array. Each applicable probe is selectively unmasked prior to bathing the array in a solution of a single nucleotide, then a masking reaction takes place and the next set of probes are unmasked in preparation for a different nucleotide exposure. After many repetitions, the sequences of every probe become fully constructed. One of the keys to this process is through probe redundancy where each array is constructed from a series of mismatch (MM) control probes that are identical to their perfect match (PM) probes, allowing some assessment of non-specific binding and performance of the probes. Overall, the Affymetrix oligonucleotide arrays allow robust measures of gene expression. The resolution allows one hundred million non-overlapping 30-mer probes spanning the entire human genome to fit on a 2x2 cm array. Furthermore, these arrays are designed based on sequence information, which eliminates the need for physical intermediates such as clones, PCR products, or cDNAs, which drastically reduces many sources of potential error.

### Altman et al. 2014

- Altman, Naomi, and Martin Krzywinski. "Points of Significance: Sources of Variation." Nature Methods 12, no. 1 (December 30, 2014): 5-6.

**Summary:**

The article discusses the double-edged sword that is variation in biological based experiments. Although general consensus is to minimize experimental variability, some variability needs to be maintained to allow generalization of the results. The authors argue that experimental control, randomization, blocking, and replication are the tools that allow replicable and meaningful results. Variability affects replication of experimental results (internal validity) which, in turn, affects the generalizability to a larger population (external validity). A well-designed experiment is a compromise between internal and external validity, and, with careful experimental design, the impact of biological variability can be reduced. Genotype and gender are examples of sources of variability that are under complete experimental control, while other sources of variability, such as diet, temperature, and housing, are under partial experimental control. So-called "noise" factors that cannot be controlled, or are unknown, can be handled by randomization, replication, and blocking. Randomization is employed to avoid biasing experimental results. Replication is needed to increase the precision with which a sample is characterized (e.g. standard error mean). However, the biological variability can be underestimated when measurements are an average of a large number of contributing factors. Blocking on a noise variable removes its effect by taking a difference of two measurements that share the same value of the noise, effectively isolating this variability source. Technical variability, such as reagents, measurement platforms, and personnel, also contribute to the overall variation. Steps should be taken to reduce sources of variation that are nuisance factors while maintaining the sources of variation required to assess how much effects vary in the generalizable population. Whereas the former should be minimized to optimize the power of the experiment, the latter need to be sampled and quantified so that results can be generalized while maintaining the integrity of the estimate uncertainty.

### Bolstad et al. 2003

- Bolstad, Irizarray, Astrand, Speed. "A Comparison of Normalization Methods for High Density Oligonucleotide Array Data Based on Variance and Bias." Bioinformatics 19, no.2 (January 22, 2003): 185-93.

**Summary:**

The article compares the performance of three methods of normalization for use when analyzing high density oligonucleotide arrays. It is important to remove sources of variation between arrays of non-biological origin when running experiments that involve multiple high density oligonucleotide arrays. Normalization is a process for reducing unwanted variability. For these arrays, oligonucleotides of 25 base pairs in length are used to probe genes. There are two types of probes: 1) perfect match (PM) probes that match a target sequence exactly and 2) mismatch (MM) probes, which differ from the reference probes only by a single base in the center of the sequence. The normalization methods are performed at the probe intensity level, and are referred to as complete data methods because they make use of data from all arrays in an experiment to form the normalizing relation. These methods are then compared to two methods that make use of a baseline array. The first method is Cyclic loess. It is an approach based upon the idea of the M versus A plot, where M is the difference in log expression values and A is the average of the log expression values. The second method is contrast. The contrast based method is another extension of the M versus A method. Quantile normalization is the last method where the goal is to make the distribution of probe intensities for each array in a set of arrays the same. The results of the study suggest that all three of the complete data methods reduced the variation of the probeset measure across a set of arrays to a greater degree than their reference methods. However, the quantile method performed favorably across all experiments, both in terms of speed and when using their proposed variance and bias criteria.

### Cleveland et al. 1988

- Cleveland and Devlin. "Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting." Journal of the American Statistical Association 83, no. 403 (1988): 596-610.

**Summary:**

The article discusses locally weighted regression (loess). It is a way of estimating a regression surface through a multivariate smoothing procedure, fitting a function of the independent variables locally and in a moving fashion analogous to how a moving average is computed for a time series. Interestingly, the term loess is derived from the word meaning a deposit of fine clay or silt along river valleys; in a vertical cross-section of Earth, a loess would appear as a narrow, curve-like stratum running through the section. A much wider class of smooth functions can be estimated using local fitting. The applications in the present article illustrate three major uses of the local-fitting methodology. The first is simply to provide an exploratory graphical tool; graphing smooth surfaces that are fitted to the data can give insight into the behavior of the data and help choose parametric models. The second is to provide additional regression diagnostics to check the adequacy of parametric models fitted to the data. The third is to use the loess estimate as the estimated regression surface, without resorting to a parametric class of functions. Other methodologies are discussed, which include multivariate smoothing, the statistical properties of loess that are analogous to those used in the least-squares fitting of parametric functions, several graphical methods that are useful tools for understanding loess estimates and checking assumptions on which the estimation procedure is based, and the M plot.

### Tusher et al. 2001

- Tusher, Tibshirani, and Chu. "Significance analysis of microarrays applied to the ionizing radiation response." PNAS 98, no. 9 (2001): 5116 - 21.

**Summary:**

In this article, the authors describe a method, Significance Analysis of Microarrays (SAM), which determines the significance of changes in gene expression between different biological states determined by microarray while accounting for the enormous number of genes measured. Methods based on conventional t tests provide the probability that a difference in gene expression occurred by chance. Although P= 0.01 is significant in the context of experiments designed to evaluate small numbers of genes, a microarray experiment for 10,000 genes would identify 100 genes by chance which is problematic. The authors developed SAM, which identifies genes with statistically significant changes in expression by assimilating a set of gene-specific t tests. Each gene is assigned a score based on its change in gene expression relative to the standard deviation of repeated measurements for that gene. For genes with scores greater than an adjustable threshold, SAM uses permutations of the repeated measurements to estimate the percentage of genes identified by chance, which is the false discovery rate (FDR). During multiple testing, the family-wise error rate is the probability of at least one false positive over the collection of tests. SAM does not have a strong or weak control over the family-wise error rate, and, instead, provides an estimate of the FDR for each value of the tuning parameter define within the algorithm. The authors deem it plausible that the estimated FDR approximates the strongly controlled FDR when any subset of null hypotheses is true. The article demonstrates the efficacy of SAM by assessing the transcriptional response of lymphoblastoid cells to ionizing radiation. The results suggest that SAM is a robust and straightforward method that can be adapted to a broad range of experimental situations.

### Smyth 2004

- Smyth. “Linear Models and Empirical Bayes Methods for Assessing Differential Expression in Microarray Experiments.” Statistical Applications in Genetics and Molecular Biology 3, no. 1 (2004): 1 – 25.

**Summary:**

Smyth describes the problem of identifying differentially expressed genes in designed microarray experiments. The massive amount of data assessed in microarray experiments results in a multiple testing problem in which one or more tests are conducted for each of tens of thousands of genes. This is then complicated by the fact that the measured expression levels are often non-normally distributed and have non-identical and dependent distributions between genes. The author describes striking a balance between controlling the familywise error rate or false discovery rate (FDR) and the parallel nature of the inference in microarrays, which allows some compensating possibilities for borrowing information from the ensemble of genes, which can assist in inference about each gene individually. Previously, an article described and derived a method that can reconcile this issue through the application of Bayes or empirical Bayes methods. Using a replicated two-color microarray experiment, Lonnstedt and Speed (2002), took a parametric empirical Bayes approach and derived a simple expression for the posterior odds of differential expression for each gene. The model is reset in the context of general linear models with arbitrary coefficients and contrasts of interest while closed form estimators are derived for the hyperparameters in the model. Furthermore, this approach shrinks the estimated sample variance towards a pooled estimate, resulting in far more stable inference when the number of arrays is small. The method is demonstrated for simulated data, and the results indicate that the proposed estimators have robust behavior that allow for incomplete data that can sometimes arise from spot filtering or spot quality weights.

### Benjamini et al. 1995

- Benjamini and Hochberg. “Controlling the False Discovery Rate: a Practical and Powerful Approach to Multiple Testing.” J.R. Statist. Soc. B 57, no. 1 (1995): 289 – 300.

**Summary:**

The authors describe a new point of view on the problem of the multiplicity (selection) effect. In many multiplicity problems, the number of erroneous rejections should be taken into account. Far too often only the question of whether any error was made, which can lead to a serious loss incurred by erroneous rejections. The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). However, the article points out many issues that arise when using methods that control the FWER including: 1) application of multiple-treatment type tests to non-multiple treatment experiments; 2) applications of these tests tend to substantially reduce the power than the per comparison procedure of the same level; and 3) redundant application of FWER control when it is not needed. Instead, the authors describe a different approach that calls for controlling the expected proportion of falsely rejected hypotheses. This false discovery rate (FDR) is equivalent to the FWER when all hypotheses are true but is smaller otherwise. This leads to potential gain in power when controlling the FDR as opposed to the FWER. In many applications, this is the desirable control against errors originating from multiplicity. A simple sequential Bonferroni-type procedure is shown to control the FDR for independent test statistics. The results demonstrate that controlling the FDR is both simple and powerful; however, it does not take into the structure of specific problems such as pairwise comparisons in analysis of variance.

### Storey et al. 2003

- Storey and Tibshirani. “Statistical Significance for Genomewide Studies.” PNAS 100, no. 16 (2003): 9440 – 45.

**Summary:**

Genomewide experiments and the sequencing of multiple genomes have become commonplace, which consequently has raised issues pertaining to the analyses of large data sets. The analyses of these data also involve performing statistical tests on thousands of features in a genome. Prior to performing the analysis, it is expected that many more than one or two of the tested features will be statistically significant. Guarding against any single false positive occurring is often going to be much too strict and will lead to many missed findings. The authors propose an approach to measuring statistical significance in these genomewide studies based on the concept of the false discovery rate (FDR). The goal is to propose and estimate a measure of significance for each feature that meets the practical goals of the genomewide study that is easily interpreted in terms of the simultaneous testing of thousands of features. Instead of a p-value, a q-value is associated with each tested feature. The q-value is similar to the p-value, except it is a measure of significance in terms of the FDR rather than the false positive rate. The results show that the FDR is a sensible measure of the balance between the number of true positives and false positives. Furthermore, it avoids over-inflation of false positive results, while still offering a more sensible criterion than what has been used previously for linkage studies involving genome scans.

### Hung et al. 2011

- Hung, Yang, Hu, Weng, and DeLisi. “Gene set enrichment analysis: performance evaluation and usage guidelines.” Briefings in Bioinformatics 13, no. 3, (2011): 281 – 91.

**Summary:**

The article describes analytical methods for identifying related sets of genes associated with phenotypic changes (i.e. stage of differentiation, disease state, responsiveness to outside stimuli, etc.). Early methods for associating gene sets with phenotype changes first identify individual, potentially relevant genes by making a binary decision based on a quantity that measures the extent of differential expression between the phenotypes and then use a Fisher’s exact test to determine whether a significant number of these genes belong to a predetermined gene set. An alternative approach, named Gene Set Enrichments Analysis (GSEA), ranks all genes according to differential expression, and then determines if a predetermined gene set is significantly overrepresented. Performing GSEA is complex because of the numerous variants that are dependent on the method for estimating significance, background distribution, choice of the shuffling method, the method of multiple hypothesis correction, and the choice of weights to account for auxiliary information. The lack of a gold standard has greatly hampered the effort of assessing gene set enrichment methods using experimental data sets. Simulated data cannot substitute experimental data, as biological systems are so complex. The authors use GSEA to analyze 132 experimental data sets to assess six gene set-level statistics and one global test. The results presented in the paper suggest that the Wilcoxon rank sum test and the WKS test as implemented in GSEA provide the most effective gene set-level statistics for obtaining high mutual coverage and incorporating topology of gene sets in the analysis increases the sensitivity for all procedures. Furthermore, simulated background distributions are more accurate than analytic backgrounds and the mean and median tests achieve the highest sensitivity but poor mutual coverage.

### Bard et al. 2004

- Bard and Rhee. “Ontologies in Biology: Design, Applications and Future Challenges.” Nature Reviews Genetics, 5, (2004): 213 – 22.

**Summary:**

Ontologies are a formal way of representing knowledge in which concepts are described both by their meaning and by their relationship to each other. Biological based ontologies (so called bio-ontologies) can be used for linking to and querying molecular databases. The article reviews the principal bio-ontologies and the current issues in their design and development. Ontologies are different from annotations in that they formalize the meaning of terms through a set of assertions and rules that are collectively known as a description logic. An advantage of ontologies is that the description logic can be used for querying an information set and for facilitating analyses across information sets that are not traditionally accessible to searching and comparing. Ontologies have been used in biology for some time. The Gene Ontology (GO) is by far the most widely used bio-ontology. It aims to formalize our knowledge about biological processes, molecular functions and cell components, in three orthogonal hierarchies. At the time of publication of the article, Cell Ontology was a new and still unfinished ontology, which was designed to provide all model species with a common language and ID set for cell phenotypes. These bio-ontologies have a wide variety of uses, the most important of which is the representation of knowledge in a computer-comprehensible way, interoperability across databases, and the annotation and analysis of large-scale data with ontology IDs. However, integrating this knowledge poses two problems. First, not everyone in a field agrees on either the facts or the relationships. Second, knowledge changes with time, even in apparently ossified subjects such as anatomy. If ontologies are properly curated over the longer term they will come to be seen as modern-day textbooks providing online and up-to-date biological expertise for their area.
